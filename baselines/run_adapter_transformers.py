# coding=utf-8
# Copyright 2020 The HuggingFace Team All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Fine-tuning the library models for token classification.
"""

import logging
import io
import os
import sys
from pathlib import Path
from zipfile import ZipFile
from dataclasses import dataclass, field
from typing import Optional

import numpy as np
from datasets import load_dataset, Dataset

import transformers
from transformers import (
    AdapterConfig,
    AdapterType,
    AutoConfig,
    AutoModelForTokenClassification,
    AutoTokenizer,
    DataCollatorForTokenClassification,
    HfArgumentParser,
    MultiLingAdapterArguments,
    Trainer,
    TrainingArguments,
    set_seed,
)
from transformers.trainer_utils import is_main_process

logger = logging.getLogger(__name__)


@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
    """

    model_name_or_path: str = field(
        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
    )
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    cache_dir: Optional[str] = field(
        default=None, metadata={"help": "Where do you want to store the pretrained models downloaded from s3"}
    )


@dataclass
class DataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """

    task_name: Optional[str] = field(default="ner", metadata={"help": "The name of the task (ner, pos...)."})
    dataset_name: Optional[str] = field(
        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
    )
    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
    )
    train_file: Optional[str] = field(
        default=None, metadata={"help": "The input training data file (a csv or JSON file)."}
    )
    validation_file: Optional[str] = field(
        default=None,
        metadata={"help": "An optional input evaluation data file to evaluate on (a csv or JSON file)."},
    )
    test_file: Optional[str] = field(
        default=None,
        metadata={"help": "An optional input test data file to predict on (a csv or JSON file)."},
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "The number of processes to use for the preprocessing."},
    )
    pad_to_max_length: bool = field(
        default=False,
        metadata={
            "help": "Whether to pad all samples to model maximum sentence length. "
            "If False, will pad the samples dynamically when batching to the maximum length in the batch. More "
            "efficient on GPU but very bad for TPU."
        },
    )
    label_all_tokens: bool = field(
        default=False,
        metadata={
            "help": "Whether to put the label for one word on all tokens of generated by that word or just on the "
            "one (in which case the other tokens will have a padding index)."
        },
    )

    def __post_init__(self):
        if self.dataset_name is None and self.train_file is None and self.validation_file is None:
            raise ValueError("Need either a dataset name or a training/validation file.")
        else:
            if self.train_file is not None:
                extension = self.train_file.split(".")[-1]
                assert extension in ["csv", "json"], "`train_file` should be a csv or a json file."
            if self.validation_file is not None:
                extension = self.validation_file.split(".")[-1]
                assert extension in ["csv", "json"], "`validation_file` should be a csv or a json file."
        self.task_name = self.task_name.lower()


def main():
    # See all possible arguments in src/transformers/training_args.py
    # or by passing the --help flag to this script.
    # We now keep distinct sets of args, for a cleaner separation of concerns.

    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments, MultiLingAdapterArguments))
    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
        # If we pass only one argument to the script and it's the path to a json file,
        # let's parse it to get our arguments.
        model_args, data_args, training_args, adapter_args = parser.parse_json_file(
            json_file=os.path.abspath(sys.argv[1])
        )
    else:
        model_args, data_args, training_args, adapter_args = parser.parse_args_into_dataclasses()

    if (
        os.path.exists(training_args.output_dir)
        and os.listdir(training_args.output_dir)
        and training_args.do_train
        and not training_args.overwrite_output_dir
    ):
        raise ValueError(
            f"Output directory ({training_args.output_dir}) already exists and is not empty."
            "Use --overwrite_output_dir to overcome."
        )

    # Setup logging
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO if is_main_process(training_args.local_rank) else logging.WARN,
    )

    # Log on each process the small summary:
    logger.warning(
        f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
        + f"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
    )
    # Set the verbosity to info of the Transformers logger (on main process only):
    if is_main_process(training_args.local_rank):
        transformers.utils.logging.set_verbosity_info()
    logger.info("Training/evaluation parameters %s", training_args)

    training_args.num_train_epochs = 20.0
    training_args.save_steps = 1000
    training_args.per_device_train_batch_size = 8
    training_args.per_device_eval_batch_size = 8

    # Set seed before initializing model.
    set_seed(training_args.seed)
    datasets = load_dataset('hf_sepp_nlg_dataset_en.py')
    text_column_name = "tokens"
    label_column_name = "ner_tags"

    # Labeling (this part will be easier when https://github.com/huggingface/datasets/issues/797 is solved)
    def get_label_list(labels):
        unique_labels = set()
        for label in labels:
            unique_labels = unique_labels | set(label)
        label_list = list(unique_labels)
        label_list.sort()
        return label_list

    label_list = get_label_list(datasets["train"][label_column_name])
    str_label_list = ['.', ',', '?', '-', ':', '0']
    label_to_id = {l: i for i, l in enumerate(label_list)}
    num_labels = len(label_list)

    config = AutoConfig.from_pretrained(
        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
        num_labels=num_labels,
        finetuning_task=data_args.task_name,
        cache_dir=model_args.cache_dir,
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
        cache_dir=model_args.cache_dir,
        use_fast=True,
    )
    model = AutoModelForTokenClassification.from_pretrained(
        model_args.model_name_or_path,
        from_tf=bool(".ckpt" in model_args.model_name_or_path),
        config=config,
        cache_dir=model_args.cache_dir,
    )

    # Setup adapters
    task_name = data_args.dataset_name or "sepp_nlg_2021"
    if adapter_args.train_adapter:
        # check if adapter already exists, otherwise add it
        if task_name not in model.config.adapters.adapter_list(AdapterType.text_task):
            # resolve the adapter config
            adapter_config = AdapterConfig.load(
                adapter_args.adapter_config,
                non_linearity=adapter_args.adapter_non_linearity,
                reduction_factor=adapter_args.adapter_reduction_factor,
            )
            # load a pre-trained from Hub if specified
            if adapter_args.load_adapter:
                model.load_adapter(
                    adapter_args.load_adapter,
                    AdapterType.text_task,
                    config=adapter_config,
                    load_as=task_name,
                )
            # otherwise, add a fresh adapter
            else:
                model.add_adapter(task_name, AdapterType.text_task, config=adapter_config)
        # optionally load a pre-trained language adapter
        if adapter_args.load_lang_adapter:
            # resolve the language adapter config
            lang_adapter_config = AdapterConfig.load(
                adapter_args.lang_adapter_config,
                non_linearity=adapter_args.lang_adapter_non_linearity,
                reduction_factor=adapter_args.lang_adapter_reduction_factor,
            )
            # load the language adapter from Hub
            lang_adapter_name = model.load_adapter(
                adapter_args.load_lang_adapter,
                AdapterType.text_lang,
                config=lang_adapter_config,
                load_as=adapter_args.language,
            )
        else:
            lang_adapter_name = None
        # Freeze all model weights except of those of this adapter
        model.train_adapter([task_name])
        # Set the adapters to be used in every forward pass
        if lang_adapter_name:
            model.set_active_adapters([lang_adapter_name, task_name])
        else:
            model.set_active_adapters([task_name])
    else:
        if adapter_args.load_adapter:
            task_name = data_args.dataset_name or "ner"
            adapter_config = AdapterConfig.load(
                adapter_args.adapter_config,
                non_linearity=adapter_args.adapter_non_linearity,
                reduction_factor=adapter_args.adapter_reduction_factor,
            )
            # load a pre-trained from Hub if specified
            adapter_name = model.load_adapter(
                adapter_args.load_adapter,
                AdapterType.text_task,
                config=adapter_config,
                load_as=task_name,
            )
            model.set_active_adapters(adapter_name)

    # Preprocessing the dataset
    # Padding strategy
    padding = "max_length" if data_args.pad_to_max_length else False

    # Tokenize all texts and align the labels with them.
    def tokenize_and_align_labels(examples):
        tokenized_inputs = tokenizer(
            examples[text_column_name],
            padding=padding,
            truncation=True,
            # We use this argument because the texts in our dataset are lists of words (with a label for each word).
            is_split_into_words=True,
            return_offsets_mapping=True,
        )
        offset_mappings = tokenized_inputs.pop("offset_mapping")
        labels = []

        for label, offset_mapping in zip(examples[label_column_name], offset_mappings):
            label_index = 0
            current_label = -100
            label_ids = []
            for offset in offset_mapping:
                # We set the label for the first token of each word. Special characters will have an offset of (0, 0)
                # so the test ignores them.
                if offset[0] == 0 and offset[1] != 0:
                    current_label = label_to_id[label[label_index]]
                    label_index += 1
                    label_ids.append(current_label)
                # For special tokens, we set the label to -100 so it's automatically ignored in the loss function.
                elif offset[0] == 0 and offset[1] == 0:
                    label_ids.append(-100)
                # For the other tokens in a word, we set the label to either the current label or -100, depending on
                # the label_all_tokens flag.
                else:
                    label_ids.append(current_label if data_args.label_all_tokens else -100)
            labels.append(label_ids)

        tokenized_inputs["labels"] = labels
        return tokenized_inputs

    # Data collator
    data_collator = DataCollatorForTokenClassification(tokenizer)

    if training_args.do_train:
        tokenized_datasets = datasets.map(
            tokenize_and_align_labels,
            batched=True,
            num_proc=data_args.preprocessing_num_workers,
            load_from_cache_file=not data_args.overwrite_cache
        )

        # Metrics
        def compute_metrics(p):
            return {'dummy': 0}

        # Initialize our Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_datasets["train"] if training_args.do_train else None,
            eval_dataset=tokenized_datasets["validation"] if training_args.do_eval else None,
            tokenizer=tokenizer,
            data_collator=data_collator,
            compute_metrics=compute_metrics,
            do_save_full_model=not adapter_args.train_adapter,
            do_save_adapters=adapter_args.train_adapter,
        )

        # Training
        if training_args.do_train:
            trainer.train(
                model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None
            )
            trainer.save_model()  # Saves the tokenizer too for easy upload

        # Evaluation
        if training_args.do_eval:
            logger.info("*** Evaluate ***")
            results = trainer.evaluate()
            output_eval_file = os.path.join(training_args.output_dir, "eval_results_ner.txt")
            if trainer.is_world_process_zero():
                with open(output_eval_file, "w") as writer:
                    logger.info("***** Eval results *****")
                    for key, value in results.items():
                        logger.info(f"  {key} = {value}")
                        writer.write(f"{key} = {value}\n")

    # Predict
    if training_args.do_predict:

        logger.info("*** Predict ***")
        lang, data_set = 'en', 'test'
        outdir = os.path.join(training_args.output_dir, lang, data_set)
        Path(outdir).mkdir(parents=True, exist_ok=True)
        data_zip = 'data/sepp_nlg_2021_data_v5.zip'
        #data_zip = 'data/sepp_nlg_2021_surprise_testset.zip'
        max_len = int(512 * 0.75)

        model.eval()  # remove dropout etc.

        predictor = Trainer(
            model=model,
            args=training_args,
            tokenizer=tokenizer,
            data_collator=data_collator
        )

        with ZipFile(data_zip, 'r') as zf:
            fnames = zf.namelist()
            relevant_dir = os.path.join('sepp_nlg_2021_data', lang, data_set)
            tsv_files = [
                fname for fname in fnames
                if fname.startswith(relevant_dir) and fname.endswith('.tsv')
            ]

            for tsv_file in tsv_files:
                print('\r' + tsv_file, end='')
                with io.TextIOWrapper(zf.open(tsv_file), encoding="utf-8") as f:
                    tsv_str = f.read()
                tsv_lines = tsv_str.strip().split('\n')
                lines = [line.strip().split('\t') for line in tsv_lines]
                tokens = [line[0].replace('\xad', '').replace('\u200b','') for line in lines]  # remove soft hyphen

                batches = [tokens[split_ix: split_ix + max_len]  # segment token list into lists of max_len
                           for split_ix in range(0, len(tokens), max_len)]
                test_data = {'id': list(range(len(batches))), 'tokens': batches,
                             'ner_tags': [[0 * len(toks) for toks in batch] for batch in batches]}  # dummy labels
                tokenized_test_data = tokenize_and_align_labels(test_data)
                test_dataset = Dataset.from_dict(tokenized_test_data)
                predictions, labels, metrics = predictor.predict(test_dataset)
                predictions = np.argmax(predictions, axis=2)
                true_predictions = [
                    [str_label_list[p] for (p, l) in zip(prediction, label) if l != -100]
                    for prediction, label in zip(predictions, labels)
                ]
                all_preds = [tok for pred in true_predictions for tok in pred]

                if not len(all_preds) == len(tokens):
                    # reduce max_len until the output isn't truncated (due to subword tokenization)
                    new_max_len = max_len
                    while not len(all_preds) == len(tokens) and new_max_len > 3:
                        new_max_len = int(new_max_len * .75)
                        batches = [tokens[split_ix: split_ix + new_max_len]
                                   for split_ix in range(0, len(tokens), new_max_len)]
                        test_data = {'id': list(range(len(batches))), 'tokens': batches,
                                     'ner_tags': [[0 * len(toks) for toks in batch] for batch in
                                                  batches]}  # dummy labels
                        tokenized_test_data = tokenize_and_align_labels(test_data)
                        test_dataset = Dataset.from_dict(tokenized_test_data)
                        predictions, labels, metrics = predictor.predict(test_dataset)
                        predictions = np.argmax(predictions, axis=2)
                        true_predictions = [
                            [str_label_list[p] for (p, l) in zip(prediction, label) if l != -100]
                            for prediction, label in zip(predictions, labels)
                        ]
                        all_preds = [tok for pred in true_predictions for tok in pred]
                    if not len(all_preds) == len(tokens):
                        print('miss-match prediction length')
                        import pdb; pdb.set_trace()

                with open(os.path.join(outdir, os.path.basename(tsv_file)), 'w',
                          encoding='utf8') as of:
                    for tok_str, label_st2 in zip(tokens, all_preds):
                        label_st1 = 1 if label_st2 in {'.', '?'} else 0
                        of.write(f'{tok_str}\t{label_st1}\t{label_st2}\n')
            print()


def _mp_fn(index):
    # For xla_spawn (TPUs)
    main()


if __name__ == "__main__":
    main()
